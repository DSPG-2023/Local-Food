{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "#Imports for Scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from os import path\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "#Creator's Note: Products(Enum) and ProductsLoader is probably the only classes you need to edit \n",
    "#unless you need to change the way the data is cleaned. Which handled in the DataCleaner class\n",
    "\n",
    "#These class is here so that we can expand to differnet products easier making the spider more dynamic and expandable\n",
    "class Products(Enum):\n",
    "    #Add products like this ProductName = index iteration, [], [] \n",
    "    #the 2 empty list will be filled in using the ProductsLoader class\n",
    "    Bacon = 0, [], []\n",
    "    Eggs = 1, [], []\n",
    "    HeirloomTomatoes = 2, [], []\n",
    "\n",
    "    # Helper method to reduce code for adding to the products and weed out duplicate inputs\n",
    "    # if you type something in really wrong code will stop the setup is important \n",
    "    # correct index inputs are correct index number, url, urls, xpath, xpaths\n",
    "    def addToProduct(self, items, index):\n",
    "        product = None\n",
    "        if isinstance(index, int):\n",
    "            product = self.value[index]\n",
    "        elif isinstance(index, str):\n",
    "            if index.lower() in ['urls', 'url']:\n",
    "                product = self.value[1]\n",
    "            elif index.lower() in ['xpaths', 'xpath']:\n",
    "                product = self.value[2]\n",
    "        if product == None:\n",
    "            raise ValueError(f\"Invalid index input for ({index}) for input: {items}\")\n",
    "        #Sets are fast at finding dups so we use them for speed\n",
    "        product_set = set(product)\n",
    "        for item in items:\n",
    "            if item not in product_set:\n",
    "                product.append(item)\n",
    "                product_set.add(item)\n",
    "\n",
    "#This class loads the xpaths and urls to the Products Enum and adds dataframes to the spider\n",
    "class ProductsLoader():\n",
    "    DataFrames = []\n",
    "    storeXpaths = []\n",
    "    def __init__(self):\n",
    "        self.dataFrameAdder()\n",
    "        self.setStoreXpaths()\n",
    "        self.urlsAdder()\n",
    "        self.xpathMaker()\n",
    "\n",
    "    #This adds the dataframe to the spider on load\n",
    "    def dataFrameAdder(self):\n",
    "        #Dataframes (You can add more here)\n",
    "        baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Weight' ,'Sale', 'Store Location', 'Url'])\n",
    "        eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Weight' ,'Sale', 'Store Location', 'Url'])\n",
    "        tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Weight' ,'Sale', 'Store Location', 'Url'])\n",
    "        self.DataFrames = [baconFrame,\n",
    "                           eggFrame,\n",
    "                           tomatoFrame\n",
    "                          ]\n",
    "\n",
    "    def setStoreXpaths(self):\n",
    "        CoralvilleButtonXpath = '//*[@id=\"store-locator\"]//*[contains(@data-store-id,\"2843\") and contains(@class,\"fp-btn fp-btn-default fp-btn-mystore \") and contains(@role,\"button\")]'\n",
    "        IowaCityButtonXpath = '//*[@id=\"store-locator\"]//*[contains(@data-store-id,\"2844\") and contains(@class,\"fp-btn fp-btn-default fp-btn-mystore \") and contains(@role,\"button\")]'\n",
    "        CedarRapidsButtonXpath = '//*[@id=\"store-locator\"]//*[contains(@data-store-id,\"2845\") and contains(@class,\"fp-btn fp-btn-default fp-btn-mystore \") and contains(@role,\"button\")]'\n",
    "        self.storeXpaths = [CedarRapidsButtonXpath,\n",
    "                            IowaCityButtonXpath,\n",
    "                            CoralvilleButtonXpath\n",
    "                            ]\n",
    "\n",
    "    #Adding Urls to products\n",
    "    def urlsAdder(self):\n",
    "        BaconUrls = ['https://shop.newpi.coop/shop/meat/bacon/sliced/applegate_natural_hickory_smoked_uncured_sunday_bacon_8_oz/p/19959#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/beeler_hickory_smoked_bacon/p/7703726#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/beeler_bacon_ends_and_pieces/p/1564405684703446698#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/beeler_hickory_smoked_bacon/p/7791059#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/garrett_valley_pork_bacon_classic_dry_rubbed_uncured/p/7703238#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/turkey/garrett_valley_turkey_bacon_sugar_free_paleo/p/7703237#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/beeler_pepper_bacon/p/1564405684702577823#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/turkey/plainville_farms_turkey_bacon_uncured/p/4750634#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/garrett_valley_pork_bacon_8_oz/p/6572556#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/refrigerated/meat_alternatives/herbivorous_butcher_hickory_maple_bacon/p/1564405684704334152#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/pork/new_pi_bulk_bacon/p/1564405684704337543#!/?department_id=1322171',\n",
    "                     'https://shop.newpi.coop/shop/meat/pork/niman_ranch_uncured_bacon_12_oz/p/7276#!/?department_id=1322171'\n",
    "                    ]\n",
    "        EggUrls = ['https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_grade_a_free_range_large_brown_12_ea/p/7110637',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_white_cage_free_large/p/7110638',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_large_brown_free_range/p/7613595',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/vital_farms_eggs_organic_pasture_raised_large_12_ea/p/5637123',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_dozen_xtra_large/p/1564405684714084840',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_jumbo_brown/p/7613596',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_large_eggs/p/1564405684704338616',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_og_pasture_lrg_brwn/p/7613597',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_large_brown_free_range/p/1564405684703497142',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/vital_farms_large_pasture_raised_eggs_12_ea/p/5323128',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/vital_farms_eggs_pasture_raised_large_18_ea/p/1564405684690018196',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/organic_valley_free_range_brown_large_eggs_12_ea/p/48765',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_extra_large_eggs/p/1564405684704338617',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/steinecke_family_farm_duck_eggs/p/1564405684711593802',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cosgrove_rd_farm_eggs_pasture_raised/p/1564405684710338102',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_jumbo_eggs/p/1564405684704338619',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_large/p/1564405684704684702',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_medium/p/1564405684713746940',\n",
    "                  ]\n",
    "        HeirloomTomatoesUrls = ['https://shop.newpi.coop/shop/produce/fresh_vegetables/tomatoes/heirloom_tomatoes/p/2311736']\n",
    "\n",
    "        Products.Bacon.addToProduct(BaconUrls,'urls')\n",
    "        Products.Eggs.addToProduct(EggUrls,'urls')\n",
    "        Products.HeirloomTomatoes.addToProduct(HeirloomTomatoesUrls,'urls')\n",
    "\n",
    "    #This handles the xpaths by adding to the Products class\n",
    "    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item \n",
    "    #if that is the case\n",
    "    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page \n",
    "    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value \n",
    "    #and hurt the preformence\n",
    "    #best practice is to render the optional last so it reduces the chances of skipping \n",
    "    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one \n",
    "    #is by using skipHandler method and tracking/watching the logs  \n",
    "    #IMPORTANT < -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!->\n",
    "    def xpathMaker(self):\n",
    "        #Add the xpaths here and mark if they are optional\n",
    "        nameXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-name\")]' #special because the store can be not carrying the product\n",
    "        priceXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-price\")]//span[contains(@class,\"fp-item-base-price\")]'\n",
    "        weightXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-price\")]//span[contains(@class,\"fp-item-size\")]'\n",
    "        saleXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-sale\")]//span[contains(@class,\"fp-item-sale-price\")]' # optional\n",
    "        #xpath, Optional, special\n",
    "        xpathList = [(nameXpath, False, True),\n",
    "                     (priceXpath, False),\n",
    "                     (weightXpath, False),\n",
    "                     (saleXpath, True)]\n",
    "\n",
    "        Products.Bacon.addToProduct(xpathList,'xpath')\n",
    "        Products.Eggs.addToProduct(xpathList,'xpath')\n",
    "        Products.HeirloomTomatoes.addToProduct(xpathList,'xpath')\n",
    "\n",
    "\n",
    "class DataCleaner():\n",
    "    DataArray = []\n",
    "    storeLocation = ''\n",
    "    def cleanUp(self, item, url):\n",
    "        self.DataArray = item\n",
    "        self.DataArray.append(self.storeLocation)\n",
    "        self.DataArray.append(url)\n",
    "        return self.DataArray\n",
    "    \n",
    "    def setStore(self, storeIndex):\n",
    "        cases = {\n",
    "            0: \"3338 Center Point Road Northeast Cedar Rapids, IA 52402\",\n",
    "            1: \"22 South Van Buren Street Iowa City, IA 52240\",\n",
    "            2: \"1101 2nd St Coralville, IA 52241\"\n",
    "        }\n",
    "        self.storeLocation = cases.get(storeIndex)\n",
    "\n",
    "    \n",
    "\n",
    "class NewPioneerSpider():\n",
    "    name = \"New Pioneer Co-op\"  #The store name \n",
    "    spiderLogs = []         #The logs \n",
    "    skipped = []            #Skipped data \n",
    "\n",
    "    #These are methods that are available for your convences\n",
    "    def log(self, *args):\n",
    "        self.spiderLogs.append(('Logger:', args))\n",
    "        if self.LOGGER:\n",
    "            print('Logger:', *args)\n",
    "\n",
    "    def debug(self, *args):\n",
    "        self.spiderLogs.append(('Debug:', args))\n",
    "        if self.DEBUGGER:\n",
    "            print('Debug:', *args)\n",
    "    \n",
    "    def printer(self, *args):\n",
    "        self.spiderLogs.append(('Printer:', args))\n",
    "        print(*args)\n",
    "    \n",
    "    def printLogs(self):\n",
    "        print(\"\\n< --- Printing Logs --- >\\n\")\n",
    "        for entry in self.spiderLogs:\n",
    "            print(*entry)\n",
    "\n",
    "    def Logs_to_file(self, filename):\n",
    "        with open(filename, 'w') as file:\n",
    "            for log_entry in self.spiderLogs:\n",
    "                file.write('{} {}\\n'.format(log_entry[0], log_entry[1]))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False\n",
    "        self.LOGGER = False #When you need to see everything that happends. The Default is False\n",
    "        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3\n",
    "        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10\n",
    "        self.count = 0 #This saves the location of the url we are going through\n",
    "        self.runTime = 0 #Total time of extractions\n",
    "        self.totalRecoveries = 0 #Number of recoveries made while running\n",
    "        self.maxRetryCount = 100 #Number of retrys the javascript can make Defualt is 100\n",
    "        self.cleaner = DataCleaner() #Loads the cleaner\n",
    "        self.load = ProductsLoader() #Loads all products\n",
    "        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too\n",
    "        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n",
    "        self.log(\"Driver started\")\n",
    "    \n",
    "    #This handles the restart in case we run into an error\n",
    "    def restart(self):\n",
    "        self.driver.quit()\n",
    "        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n",
    "        self.log(\"Driver restarted\")\n",
    "        self.setStoreLocation()\n",
    "    \n",
    "    #Some stores need to have a location set\n",
    "    def setStoreLocation(self):\n",
    "        storeLocationUrl = 'https://shop.newpi.coop/my-store/store-locator'\n",
    "        self.driver.get(storeLocationUrl)\n",
    "        time.sleep(5) #Wait for the page to set\n",
    "        xpath = self.load.storeXpaths[self.storeIndex]\n",
    "        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
    "        elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n",
    "        elements[0].click()\n",
    "        time.sleep(5) #Wait for the page to set\n",
    "        self.log(\"Store location set\")\n",
    "\n",
    "\n",
    "    #This starts the spider\n",
    "    def start_requests( self ):\n",
    "        self.runTime = time.time()\n",
    "        self.log(\"Loading from ProductsLoader Class\")\n",
    "        self.dataFrames = self.load.DataFrames #Adds all dataframes\n",
    "        self.debug(\"Products Loaded and Data Frames Added\")\n",
    "        self.debug('\\n < --- Setup runtime is %s seconds --- >' % (time.time() - self.runTime))\n",
    "        self.totalRecoveries = 0 \n",
    "        #This sweeps through every inputed store\n",
    "        for index in range(len(self.load.storeXpaths)):\n",
    "            self.storeIndex = index\n",
    "            self.cleaner.setStore(self.storeIndex)\n",
    "            self.setStoreLocation()\n",
    "            #Sweeps through all products\n",
    "            for product in (Products):\n",
    "                result = self.requestExtraction(product)\n",
    "            self.debug(\"New store location data added\")\n",
    "\n",
    "        #Adds the date that the data was scraped\n",
    "        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n",
    "        self.log(\"Exporting files\")\n",
    "        #Dataframes to CSV files\n",
    "        for df, product in zip(self.dataFrames, (Products)):\n",
    "            df.to_csv(currentDate + self.name +\" \" + product.name + \".csv\")\n",
    "            self.log('\\n', df.to_string())\n",
    "        self.debug('\\n < --- Total runtime took %s seconds with %d recoveries --- >' % (time.time() - self.runTime, self.totalRecoveries))\n",
    "        if len(self.skipped) != 0:\n",
    "            self.debug('\\n < -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND --->')\n",
    "        self.Logs_to_file(currentDate + self.name + ' Spider Logs.txt')\n",
    "        if len(self.skipped) > 0:\n",
    "            self.debug(self.skipped)\n",
    "            self.skipHandler(currentDate)      \n",
    "        self.driver.quit()\n",
    "\n",
    "    #This handles the extraction request for the inputed product \n",
    "    def requestExtraction(self, product):\n",
    "        self.count = 0\n",
    "        errors = 0\n",
    "        start = time.time()\n",
    "        self.debug(\"Starting \"+ product.name)    \n",
    "        for trying in range(self.attempts):\n",
    "            try:\n",
    "                self.makeRequest(product)\n",
    "                self.debug(product.name + \" Finished\")    \n",
    "                self.log('\\n< --- ' + product.name + ' scrape took %s seconds with %d recoveries --- >\\n' % ((time.time() - start), errors))\n",
    "                self.totalRecoveries += errors\n",
    "                return self.totalRecoveries\n",
    "            except Exception as e:\n",
    "                #Note sometimes the browser will closed unexpectedly and theres not we can do but restart the driver\n",
    "                errors += 1\n",
    "                self.debug(\"An error occurred:\", e)\n",
    "                self.debug(\"Recovering extraction and continueing\")\n",
    "                self.restart() \n",
    "        self.debug(product.name + \" Did not Finished after \" + str(self.attempts) + \" Time wasted: %s seconds\" % (time.time() - start))\n",
    "        self.totalRecoveries += errors\n",
    "        return self.totalRecoveries\n",
    "\n",
    "    #This handles the reqests for each url and adds the data to the dataframe\n",
    "    def makeRequest(self, product):\n",
    "        productUrls = product.value[1]\n",
    "        total = len(productUrls)\n",
    "        while self.count < total:\n",
    "            url = productUrls[self.count]\n",
    "            self.driver.get(url)\n",
    "            self.log(\"Making a request for: \", url)\n",
    "            item = []\n",
    "            time.sleep(1) # marionette Error Fix\n",
    "            breakout = False\n",
    "            for xpath in product.value[2]:\n",
    "                #Retrying the xpath given the number of attempts\n",
    "                for attempt in range(self.attempts):\n",
    "                    data = self.javascriptXpath(xpath[0])\n",
    "                    if data in {'empty', 'skip'}:\n",
    "                        #speical case\n",
    "                        if len(xpath) == 3:\n",
    "                            #the first attempt shouldnt go through in case it when through to fast\n",
    "                            #this will slow the fuction down however Accuracy > Speed \n",
    "                            if xpath[2]:\n",
    "                                if attempt == 0:\n",
    "                                    self.debug(\"Found a missing item in store. Double checking\")\n",
    "                                    continue\n",
    "                                #example would be when there is actually is a '' in the xpath\n",
    "                                self.debug(\"xpath marked as speical\")\n",
    "                                notFoundXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-text-center fp-not-found\")]//*[contains(@class,\"fp-text-center\")]'\n",
    "                                data = self.javascriptXpath(notFoundXpath)\n",
    "                                if data in {'empty', 'skip'}:\n",
    "                                    self.debug(\"Missing item retrying\")\n",
    "                                else:\n",
    "                                    self.debug(\"An Item not in stock for: \", url) \n",
    "                                    item.append(data)\n",
    "                                    df = self.dataFrames[product.value[0]]\n",
    "                                    num = len(df.columns) - len(item) % len(df.columns)\n",
    "                                    item += [\"None in stock\"] * (num - 2)\n",
    "                                    breakout = True\n",
    "                                    break\n",
    "                        if xpath[1] and data == 'empty':    \n",
    "                            #this is where setting the xpath to optional comes in\n",
    "                            self.debug(\"xpath wasnt avaliable\")\n",
    "                            item.append(None)\n",
    "                            break\n",
    "                        self.debug(\"Missing item retrying\")\n",
    "                    else:  #Data found\n",
    "                        item.append(data)\n",
    "                        self.log(data + ' was added to the list for: ', url)\n",
    "                        break\n",
    "                if breakout:\n",
    "                    break\n",
    "                if attempt == self.attempts:\n",
    "                    data = 'skip'\n",
    "                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n",
    "                    self.debug(\"An Item has been skipped for: \", url)  \n",
    "                    item = ['SKIPPED']\n",
    "                    #Taking the product name  dataframe number and index added as well as the url \n",
    "                    #to retry for later \n",
    "                    #This could take time to do so we do this at the very end after we made the cvs files\n",
    "                    self.skipped.append([product, self.count, url])\n",
    "                    break\n",
    "            if 'SKIPPED' in item:\n",
    "                #No point in cleaning skipped items\n",
    "                items = ['SKIPPED']*(self.dataFrames[product.value[0]].shape[1] - 1)\n",
    "                items.append(url)\n",
    "            else:\n",
    "                #We call the DataCleaner class to handle the cleaning of the data\n",
    "                #Its best to clean the data before we add it to the data frame\n",
    "                self.debug('Data cleaning started: ', item)\n",
    "                items = self.cleaner.cleanUp(item, url)\n",
    "                self.debug('Data cleaning finished: ', item)\n",
    "            self.debug('Extracted: ', items)\n",
    "            self.dataFrames[product.value[0]].loc[len(self.dataFrames[product.value[0]])] = items                    \n",
    "            self.count += 1\n",
    "            self.printer(product.name + \" item added \", self.count, \" of \", total, \":  \", items)\n",
    "\n",
    "    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python\n",
    "    #This is where selenium shines because we can both use JavaScript and render JavaScript websites\n",
    "    #and is the only reason why we use it instead of scrapy\n",
    "    def javascriptXpath(self, xpath):\n",
    "        # if the time expires it assumes xpath wasnt found in the page\n",
    "        try: \n",
    "            #Waits for page to load \n",
    "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
    "            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n",
    "\n",
    "            # Runs the javascript and collects the text data from the inputed xpath\n",
    "            # We want to keep repeating if we get any of these outputs becasue the page is still \n",
    "            # loading and we dont want to skip or waste time. (for fast computers)\n",
    "            retrycount = 0\n",
    "            invalidOutputs = {\"error\", 'skip' \"$nan\", ''}\n",
    "            while retrycount < self.maxRetryCount :\n",
    "                text = self.driver.execute_script(\"\"\"\n",
    "                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;\n",
    "                    if (!element) {\n",
    "                        return 'skip';\n",
    "                    }\n",
    "                    return element.textContent.trim();\n",
    "                \"\"\", \n",
    "                xpath)\n",
    "                checkText = text.replace(\" \", \"\").lower()\n",
    "                if checkText in invalidOutputs:\n",
    "                    retrycount+=1\n",
    "                else:\n",
    "                    self.log(retrycount, \"xpath attempts for (\", text, \")\")\n",
    "                    return text\n",
    "            self.log(\"xpath attempts count met. Problematic text (\" + text + \") for \", xpath)\n",
    "            return 'skip'\n",
    "        except TimeoutException:\n",
    "            self.log('Could not find xpath for: ', xpath)\n",
    "            return 'empty'\n",
    "\n",
    "           \n",
    "\n",
    "    #This is here to hopefully fix skipped data\n",
    "    #Best case sinarios this will never be used\n",
    "    def skipHandler(self, currentDate):\n",
    "        corrections = 0\n",
    "        # skipped format\n",
    "        # [product name, DataFrame number, DataFrame index, url]\n",
    "        while len(self.skipped) != 0:\n",
    "            #each skip \n",
    "            for index, dataSkip in enumerate(self.skipped):\n",
    "                product = dataSkip[0]\n",
    "                #Limiting the Attempts to fix while avoiding bottlenecking the problem\n",
    "                for attempt in range(self.attempts*2):\n",
    "                    product = dataSkip[0]\n",
    "                    url = dataSkip[2]\n",
    "                    self.driver.get(url)\n",
    "                    self.log(\"Making a request for: \", url)\n",
    "                    item = []\n",
    "                    breakout = False\n",
    "                    for xpath in product.value[2]:\n",
    "                        for attemptIn in range(self.attempts*2):\n",
    "                            if data in {'empty', 'skip'}:\n",
    "                                #speical case\n",
    "                                if len(xpath) == 3:\n",
    "                                    #the first attempt shouldnt go through in case it when through to fast\n",
    "                                    #this will slow the fuction down however Accuracy > Speed \n",
    "                                    if xpath[2]:\n",
    "                                        if attempt == 0:\n",
    "                                            self.debug(\"Found a missing item in store. Double checking\")\n",
    "                                            continue\n",
    "                                        #example would be when there is actually is a '' in the xpath\n",
    "                                        self.debug(\"xpath marked as speical\")\n",
    "                                        notFoundXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-text-center fp-not-found\")]//*[contains(@class,\"fp-text-center\")]'\n",
    "                                        data = self.javascriptXpath(notFoundXpath)\n",
    "                                        if data in {'empty', 'skip'}:\n",
    "                                            self.debug(\"Missing item retrying\")\n",
    "                                        else:\n",
    "                                            self.debug(\"An Item not in stock for: \", url) \n",
    "                                            item.append(data)\n",
    "                                            df = self.dataFrames[product.value[0]]\n",
    "                                            num = len(df.columns) - len(item) % len(df.columns)\n",
    "                                            item += [\"None in stock\"] * (num - 2)\n",
    "                                            breakout = True\n",
    "                                            break\n",
    "                                if xpath[1] and data == 'empty':    \n",
    "                                    #this is where setting the xpath to optional comes in\n",
    "                                    self.debug(\"xpath wasnt avaliable\")\n",
    "                                    item.append(None)\n",
    "                                    break\n",
    "                                self.debug(\"Missing item retrying\")\n",
    "                            else:  #Data found\n",
    "                                item.append(data)\n",
    "                                self.log(data + ' was added to the list for: ', url)\n",
    "                                break\n",
    "                        if breakout:\n",
    "                            break\n",
    "                    if breakout:\n",
    "                        break\n",
    "                    if attemptIn == self.attempts*2:\n",
    "                        data = 'skip'\n",
    "                        break\n",
    "                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n",
    "                    self.debug(\"Item still missing attempting other skipped for now\") \n",
    "                else:\n",
    "                    items = self.cleaner.cleanUp(item, url)\n",
    "                    self.dataFrames[dataSkip[1]].loc[dataSkip[2]] = items                    \n",
    "                    self.printer(\"Fixed \" + product.name + \" item: \", items)\n",
    "                    #To avoid infinite loops and never saving our data we save the file now\n",
    "                    self.dataFrames[product.value[0]].to_csv(currentDate + \"REPAIRED Gateway Market \" + product.name + \".csv\")\n",
    "                    self.debug('\\n < --- Total runtime with saving of repairs took %s seconds --- >' % (time.time() - self.runTime))\n",
    "                    self.Logs_to_file(currentDate + self.name + ' Spider REPAIR Logs.txt')\n",
    "                    #To avoid fixing fixed items we pop, mark, and break\n",
    "                    self.skipped.pop(index)\n",
    "                    corrections += 1\n",
    "                    break\n",
    "        self.debug('\\n < --- Total runtime with all repairs took %s seconds --- >' % (time.time() - self.runTime))\n",
    "        self.Logs_to_file(currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "#DEBUG Switch\n",
    "SHOW = True\n",
    "\n",
    "#Spider setup\n",
    "spider = NewPioneerSpider()\n",
    "spider.LOGGER = True\n",
    "spider.DEBUGGER = True\n",
    "\n",
    "#Running the spider\n",
    "spider.start_requests()\n",
    "\n",
    "if(SHOW):\n",
    "    [print(dataFrame) for dataFrame in spider.dataFrames]\n",
    "    spider.printLogs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSPG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
