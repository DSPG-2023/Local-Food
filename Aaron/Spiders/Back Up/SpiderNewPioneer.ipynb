{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import time\n",
    "import sys\n",
    "#DSPG imports to assist the spider\n",
    "from DSPG_SeleniumSpider import SeleniumSpider #Important to look at since it provides the framework\n",
    "from DSPG_Cleaner import DataCleaner # This is to handle the cleaning of data\n",
    "from DSPG_Products import Products #Imports the products to be processed\n",
    "from DSPG_SpiderErrors import DataFormatingError #Very Important\n",
    "from DSPG_SpiderErrors import StoreLocationError #Very Important\n",
    "\n",
    "# Using Products class. We only need to add the xpaths and urls since thats \n",
    "# all that really changes from spider to spider\n",
    "class ProductsLoader():\n",
    "    #index iteration, Name, Urls, Xpaths\n",
    "    Products = []\n",
    "    DataFrames = []\n",
    "\n",
    "    def __init__(self):\n",
    "        setProducts = Products()\n",
    "        self.Products = setProducts.ProductList\n",
    "        self.DataFrames = setProducts.ProductDataFrames\n",
    "        self.urlsAdder()\n",
    "        self.xpathMaker()\n",
    "        self.setStoreXpaths()\n",
    "    \n",
    "    def setStoreXpaths(self):\n",
    "        CoralvilleButtonXpath = '//*[@id=\"store-locator\"]//*[contains(@data-store-id,\"2843\") and contains(@class,\"fp-btn fp-btn-default fp-btn-mystore \") and contains(@role,\"button\")]'\n",
    "        IowaCityButtonXpath = '//*[@id=\"store-locator\"]//*[contains(@data-store-id,\"2844\") and contains(@class,\"fp-btn fp-btn-default fp-btn-mystore \") and contains(@role,\"button\")]'\n",
    "        CedarRapidsButtonXpath = '//*[@id=\"store-locator\"]//*[contains(@data-store-id,\"2845\") and contains(@class,\"fp-btn fp-btn-default fp-btn-mystore \") and contains(@role,\"button\")]'\n",
    "        self.storeXpaths = [CedarRapidsButtonXpath,\n",
    "                            IowaCityButtonXpath,\n",
    "                            CoralvilleButtonXpath\n",
    "                            ]\n",
    "\n",
    "    #Adding Urls to products\n",
    "    def urlsAdder(self):\n",
    "        BaconUrls = [\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/sliced/applegate_natural_hickory_smoked_uncured_sunday_bacon_8_oz/p/19959#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/beeler_hickory_smoked_bacon/p/7703726#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/beeler_bacon_ends_and_pieces/p/1564405684703446698#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/beeler_hickory_smoked_bacon/p/7791059#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/garrett_valley_pork_bacon_classic_dry_rubbed_uncured/p/7703238#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/turkey/garrett_valley_turkey_bacon_sugar_free_paleo/p/7703237#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/beeler_pepper_bacon/p/1564405684702577823#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/turkey/plainville_farms_turkey_bacon_uncured/p/4750634#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/bacon/garrett_valley_pork_bacon_8_oz/p/6572556#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/refrigerated/meat_alternatives/herbivorous_butcher_hickory_maple_bacon/p/1564405684704334152#!/?department_id=1322093',\n",
    "                     'https://shop.newpi.coop/shop/meat/pork/new_pi_bulk_bacon/p/1564405684704337543#!/?department_id=1322171',\n",
    "                     'https://shop.newpi.coop/shop/meat/pork/niman_ranch_uncured_bacon_12_oz/p/7276#!/?department_id=1322171'\n",
    "                    ]\n",
    "        EggUrls = [\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_grade_a_free_range_large_brown_12_ea/p/7110637',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_white_cage_free_large/p/7110638',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_large_brown_free_range/p/7613595',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/vital_farms_eggs_organic_pasture_raised_large_12_ea/p/5637123',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_dozen_xtra_large/p/1564405684714084840',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_jumbo_brown/p/7613596',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_large_eggs/p/1564405684704338616',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_og_pasture_lrg_brwn/p/7613597',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_large_brown_free_range/p/1564405684703497142',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/vital_farms_large_pasture_raised_eggs_12_ea/p/5323128',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/vital_farms_eggs_pasture_raised_large_18_ea/p/1564405684690018196',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/organic_valley_free_range_brown_large_eggs_12_ea/p/48765',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_extra_large_eggs/p/1564405684704338617',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/steinecke_family_farm_duck_eggs/p/1564405684711593802',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cosgrove_rd_farm_eggs_pasture_raised/p/1564405684710338102',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_jumbo_eggs/p/1564405684704338619',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_large/p/1564405684704684702',\n",
    "                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_medium/p/1564405684713746940',\n",
    "                  ]\n",
    "        HeirloomTomatoesUrls = [\n",
    "                                'https://shop.newpi.coop/shop/produce/fresh_vegetables/tomatoes/heirloom_tomatoes/p/2311736'\n",
    "                               ]\n",
    "        TomatoesUrls = [\n",
    "                        'https://shop.newpi.coop/shop/produce/fresh_vegetables/tomatoes/organic_on_the_vine_tomato/p/2313397#!/?department_id=1322936',\n",
    "                        'https://shop.newpi.coop/shop/produce/fresh_vegetables/tomatoes/organic_slicer_tomatoes/p/7783644#!/?department_id=1322936',\n",
    "                        'https://shop.newpi.coop/shop/produce/fresh_vegetables/tomatoes/tomatillos/p/12573#!/?department_id=1322936',\n",
    "                        'https://shop.newpi.coop/shop/produce/fresh_vegetables/tomatoes/organic_darling_grape_tomatoes/p/1564405684690758978#!/?department_id=1322936',\n",
    "                        'https://shop.newpi.coop/shop/produce/fresh_vegetables/tomatoes/heirloom_tomatoes/p/2311736'\n",
    "                       ]\n",
    "\n",
    "        self.Products[0].append(BaconUrls)\n",
    "        self.Products[1].append(EggUrls)\n",
    "        self.Products[2].append(HeirloomTomatoesUrls)\n",
    "        self.Products[3].append(TomatoesUrls)\n",
    "\n",
    "\n",
    "    #This handles the xpaths by adding to the Products class\n",
    "    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item \n",
    "    #if that is the case\n",
    "    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page \n",
    "    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value \n",
    "    #and hurt the preformence\n",
    "    #best practice is to render the optional last so it reduces the chances of skipping \n",
    "    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one \n",
    "    #is by using skipHandler method and tracking/watching the logs  \n",
    "    #IMPORTANT < -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!->\n",
    "    def xpathMaker(self):\n",
    "        #Add the xpaths here and mark if they are optional\n",
    "        nameXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-name\")]' #special because the store can be not carrying the product\n",
    "        priceXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-price\")]//span[contains(@class,\"fp-item-base-price\")]'\n",
    "        weightXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-price\")]//span[contains(@class,\"fp-item-size\")]'\n",
    "        saleXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-sale\")]//span[contains(@class,\"fp-item-sale-price\")]' # optional\n",
    "        #xpath, Optional, special\n",
    "        xpathList = [(nameXpath, False, True),\n",
    "                     (priceXpath, False),\n",
    "                     (weightXpath, False),\n",
    "                     (saleXpath, True)]\n",
    "        \n",
    "        self.Products[0].append(xpathList)\n",
    "        self.Products[1].append(xpathList)\n",
    "        self.Products[2].append(xpathList)\n",
    "        self.Products[3].append(xpathList)\n",
    "        \n",
    "#We handle most of the cleaning is done in the DataCleaner class in DSPG_Cleaner\n",
    "#There are suttle differences on what is inputed from the different spiders\n",
    "class DataFormater():\n",
    "    def __init__(self):\n",
    "        self.Clean = DataCleaner()\n",
    "    \n",
    "    def cleanUp(self, input, inputIndex, url):\n",
    "        #Loading a clean dictionary\n",
    "        self.Clean.LoadDataSet(inputIndex, url)\n",
    "        #Sales\n",
    "        if input[3] != None:\n",
    "            self.Clean.Data['Current Price'] = input[3]\n",
    "            self.Clean.Data['Orignal Price'] = input[1]\n",
    "        else:\n",
    "            self.Clean.Data['Current Price'] = input[1]\n",
    "        #Common inputs\n",
    "        self.Clean.Data['Product Type'] = input[0]\n",
    "        if(inputIndex == 0): #Bacon\n",
    "            self.Clean.Data['True Weight'] = input[2]\n",
    "            self.Clean.baconModifications()\n",
    "        elif(inputIndex == 1): #Eggs\n",
    "            self.Clean.Data['True Amount'] = input[2]\n",
    "            self.Clean.eggModifications()\n",
    "        elif(inputIndex == 2 or inputIndex == 3): #Tomatoes\n",
    "            if input[2] == 'lb':\n",
    "                string = self.Clean.Data['Current Price'] + \"/lb\"\n",
    "                self.Clean.tomatoesModifications(string)\n",
    "            else:\n",
    "                self.Clean.tomatoesModifications(input[2])\n",
    "        #Add products here\n",
    "        else:\n",
    "            raise DataFormatingError(inputIndex)\n",
    "        self.setLocationalData()\n",
    "        self.Clean.cleanPricing()\n",
    "        return list(self.Clean.Data.values())\n",
    "    \n",
    "    def outOfStock(self, input, inputIndex, url):\n",
    "        self.Clean.LoadDataSet(inputIndex, url)\n",
    "        keys = list(self.Clean.Data.keys())\n",
    "        for key in keys[:-2]:\n",
    "            self.Clean.Data[key] = \"Out of Stock\"\n",
    "        self.Clean.Data['Product Type'] = input\n",
    "        self.setLocationalData()\n",
    "        return list(self.Clean.Data.values())\n",
    "\n",
    "    def setStore(self, inputStoreIndex):\n",
    "        self.storeIndex = inputStoreIndex\n",
    "    \n",
    "    #For Hyvee there are multiple stores however I could not find a way to check each store\n",
    "    #This is something to be developed (improved upon) in the future\n",
    "    def setLocationalData(self):\n",
    "        if self.storeIndex == 0:\n",
    "            self.Clean.Data['Address'] = '3338 Center Point Road Northeast'\n",
    "            self.Clean.Data['State'] = 'IA'\n",
    "            self.Clean.Data['City'] = 'Cedar Rapids'\n",
    "            self.Clean.Data['Zip Code'] = '52402'\n",
    "        elif self.storeIndex == 1:\n",
    "            self.Clean.Data['Address'] = '22 South Van Buren Street'\n",
    "            self.Clean.Data['State'] = 'IA'\n",
    "            self.Clean.Data['City'] = 'Iowa City'\n",
    "            self.Clean.Data['Zip Code'] = '52240'\n",
    "        elif self.storeIndex == 2:\n",
    "            self.Clean.Data['Address'] = '1101 2nd St'\n",
    "            self.Clean.Data['State'] = 'IA'\n",
    "            self.Clean.Data['City'] = 'Coralville'\n",
    "            self.Clean.Data['Zip Code'] = '52241'\n",
    "        #add more locations here \n",
    "        else:\n",
    "            raise StoreLocationError(self.storeIndex)\n",
    "\n",
    "class NewPioneerSpider(SeleniumSpider):\n",
    "    name = \"New Pioneer Co-op\"  #The store name \n",
    "    skipped = []            #Skipped data \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.format = DataFormater() #Loads the Formater for cleanup and formating\n",
    "        # vvv --- If you need to change default values add them below --- vvv\n",
    "        \n",
    "    #This starts the spider\n",
    "    def start_requests( self ):\n",
    "        self.runTime = time.time()\n",
    "        self.log(\"Loading from ProductsLoader Class\")\n",
    "        self.load = ProductsLoader() #Loads all products\n",
    "        self.dataFrames = self.load.DataFrames #Adds all dataframes\n",
    "        self.debug(\"Products Loaded and Data Frames Added\")\n",
    "        self.debug('\\n < --- Setup runtime is %s seconds --- >' % (time.time() - self.runTime))\n",
    "        totalRecoveries = 0  #Number of recoveries made while running\n",
    "        #This sweeps through every inputed store\n",
    "        for index in range(len(self.load.storeXpaths)):\n",
    "            self.storeIndex = index\n",
    "            self.format.setStore(self.storeIndex)\n",
    "            self.setStoreLocation()    \n",
    "            #Sweeps through all products\n",
    "            for product in self.load.Products:\n",
    "                #For testing Only\n",
    "                totalRecoveries += self.testingRequestExtraction(product)\n",
    "                # totalRecoveries += self.requestExtraction(product)\n",
    "            self.debug(\"New store location data added\")\n",
    "        self.log(\"Exporting files\")\n",
    "        #Dataframes to CSV files\n",
    "        for df, product in zip(self.dataFrames, self.load.Products):\n",
    "            self.saveDataFrame(df, self.currentDate + self.name + \" \" + product[1] + \".csv\")\n",
    "        self.debug('\\n < --- Total runtime took %s seconds with %d recoveries --- >' % (time.time() - self.runTime, totalRecoveries))\n",
    "        if len(self.skipped) != 0:\n",
    "            self.debug('\\n < -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND --->')\n",
    "        self.Logs_to_file(self.currentDate + self.name + ' Spider Logs.txt')\n",
    "        if len(self.skipped) > 0:\n",
    "            self.debug(self.skipped)\n",
    "            self.skipHandler()      \n",
    "        self.driver.quit()\n",
    "\n",
    "    #Some stores need to have a location set\n",
    "    def setStoreLocation(self):\n",
    "        for trying in range(self.attempts):\n",
    "            try:\n",
    "                storeLocationUrl = 'https://shop.newpi.coop/my-store/store-locator'\n",
    "                self.setThisUrl(storeLocationUrl)\n",
    "                time.sleep(5) #Wait for the page to set\n",
    "                xpath = self.load.storeXpaths[self.storeIndex]\n",
    "                self.clickThis(xpath)\n",
    "                time.sleep(5) #Wait for the page to set\n",
    "                self.log(\"Store location set\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                self.debug(\"An error occurred:\", e)\n",
    "                self.debug(\"Retrying store location\")\n",
    "                self.restart() \n",
    "        self.Logs_to_file('FAILED '+ self.currentDate + self.name + ' Spider Logs.txt')\n",
    "        self.driver.quit()\n",
    "        sys.exit()\n",
    "\n",
    "    #Speical case handled here\n",
    "    def handleSpeical(self, product, url):\n",
    "        notFoundXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-text-center fp-not-found\")]/h1'\n",
    "        data = self.javascriptXpath(notFoundXpath)\n",
    "        if data in {'empty', 'skip'}:\n",
    "            self.debug(\"Missing item skipping\")\n",
    "            return ['SKIPPED']\n",
    "        else:\n",
    "            self.debug(\"An Item not in stock for: \", url) \n",
    "            return self.format.outOfStock(data, product[0], url)\n",
    "\n",
    "    #This handles the reqests for each url and adds the data to the dataframe\n",
    "    def handleRequests(self, product):\n",
    "        productUrls = product[2]\n",
    "        total = len(productUrls)\n",
    "        while self.count < total:\n",
    "            url = productUrls[self.count]\n",
    "            self.driver.get(url)\n",
    "            self.log(\"Making a request for: \", url)\n",
    "            items = []\n",
    "            time.sleep(1) # marionette Error Fix\n",
    "            breakout = False\n",
    "            for xpath in product[3]:\n",
    "                data = self.makeRequest(xpath, url)\n",
    "                if data == 'speical':\n",
    "                    #Speical case handled here\n",
    "                    items = self.handleSpeical(product, url)\n",
    "                    breakout = True\n",
    "                    break\n",
    "                elif data == 'skip':  \n",
    "                    #To help clean the data we skip the item with gaps of data \n",
    "                    self.debug(\"An Item has been skipped for: \", url)  \n",
    "                    #Taking the product and index added as well as the url to retry for later \n",
    "                    #This could take time to do so we do this at the very end after we made the cvs files\n",
    "                    self.skipped.append([product, self.count, url])\n",
    "                    items = ['SKIPPED']\n",
    "                    break\n",
    "                else:\n",
    "                    #data added to item\n",
    "                    items.append(data)          \n",
    "            if 'SKIPPED' in items:\n",
    "                #No point in cleaning skipped items\n",
    "                items = ['SKIPPED']*(self.dataFrames[product[0]].shape[1] - 1)\n",
    "                items.append(url)\n",
    "            elif breakout: \n",
    "                breakout = False\n",
    "            else:\n",
    "                #We call the DataFormater class to handle the cleaning of the data\n",
    "                #Its best to clean the data before we add it to the data frame\n",
    "                self.debug('Formating Data Started: ', items)\n",
    "                items = self.format.cleanUp(items, product[0], url)\n",
    "                self.debug('Formating Data Finished: ', items)\n",
    "                if items == None:\n",
    "                    self.printer(\"Data Formater not configured to \", product[1])\n",
    "            self.debug('Extracted: ', items)\n",
    "            self.dataFrames[product[0]].loc[len(self.dataFrames[product[0]])] = items                    \n",
    "            self.count += 1\n",
    "            self.printer(product[1] + \" item added \", self.count, \" of \", total, \":  \", items)\n",
    "\n",
    "    #This is here to hopefully fix skipped data\n",
    "    #Best case sinarios this will never be used\n",
    "    def skipHandler(self):\n",
    "        corrections = 0\n",
    "        # skipped format\n",
    "        # [product, DataFrame index, url]\n",
    "        while len(self.skipped) != 0:\n",
    "            #each skip \n",
    "            for index, dataSkip in enumerate(self.skipped):\n",
    "                product = dataSkip[0]\n",
    "                url = dataSkip[2]\n",
    "                #Limiting the Attempts to fix while avoiding bottlenecking the problem\n",
    "                for attempt in range(self.attempts*2):\n",
    "                    self.driver.get(url)\n",
    "                    self.log(\"Making a request for: \", url)\n",
    "                    items = []\n",
    "                    breakout = True\n",
    "                    speicalBreak = False\n",
    "                    for xpath in product[3]:\n",
    "                        data = self.makeRequest(xpath, url)\n",
    "                        if data == 'speical':\n",
    "                            #Speical case handled here\n",
    "                            items = self.handleSpeical(product, url)\n",
    "                            speicalBreak = True\n",
    "                            if 'SKIPPED' in items:\n",
    "                                breakout = False\n",
    "                            break\n",
    "                        elif data == 'skip':  \n",
    "                            break\n",
    "                        else:\n",
    "                            #data added to item\n",
    "                            items.append(data)\n",
    "                    if breakout:\n",
    "                        if not speicalBreak:\n",
    "                            items = self.format.cleanUp(items, product[0], url)\n",
    "                            if items == None:\n",
    "                                self.printer(\"Data Formater not configured to \", product[1])\n",
    "                                break\n",
    "                        self.dataFrames[product[0]].loc[dataSkip[1]] = items                    \n",
    "                        self.printer(\"Fixed \" + product[1] + \" item: \", items)\n",
    "                        #To avoid infinite loops and never saving our data we save the file now\n",
    "                        self.saveDataFrame(self.dataFrames[product[0]], self.currentDate + \"REPAIRED \" + self.name + \" \" + product[1] + \".csv\")\n",
    "                        self.debug('\\n < --- Total runtime with saving of repairs took %s seconds --- >' % (time.time() - self.runTime))\n",
    "                        self.Logs_to_file(self.currentDate + self.name + ' Spider REPAIR Logs.txt')\n",
    "                        #To avoid fixing fixed items we pop, mark, and break\n",
    "                        self.skipped.pop(index)\n",
    "                        corrections += 1\n",
    "                        break\n",
    "                self.debug(\"Item still missing attempting other skipped for now\") \n",
    "        self.debug('\\n < --- Total runtime with all repairs took %s seconds --- >' % (time.time() - self.runTime))\n",
    "        self.Logs_to_file(self.currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "#DEBUG Switch\n",
    "SHOW = True\n",
    "\n",
    "#Spider setup\n",
    "spider = NewPioneerSpider()\n",
    "spider.LOGGER = True\n",
    "spider.DEBUGGER = True\n",
    "\n",
    "#Running the spider\n",
    "spider.start_requests()\n",
    "\n",
    "if(SHOW):\n",
    "    [print(dataFrame) for dataFrame in spider.dataFrames]\n",
    "    spider.printLogs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSPG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
